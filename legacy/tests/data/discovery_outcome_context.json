{
  "database_name": "postgres_air",
  "repository": "bprzybys-nc/postgres-sample-dbs",
  "total_files": 73,
  "matched_files": 18,
  "files": [
    {
      "path": "database_ownership.md",
      "content": "# Database Ownership and Decommissioning Documentation\n\n## Overview\n\nThis document provides comprehensive ownership information for all databases in the postgres-sample-dbs testing environment, designed to simulate realistic database decommissioning workflows.\n\n## Database Inventory\n\n| Database | Scenario Type | Criticality | Owner Team | Contact Email | Last Used | Decommissioning Risk |\n|----------|---------------|-------------|------------|---------------|-----------|---------------------|\n| periodic_table | CONFIG_ONLY | LOW | Chemistry Team | chemistry-team@company.com | 2024-02-20 | HIGH |\n| world_happiness | CONFIG_ONLY | LOW | Analytics Team | analytics-team@company.com | 2024-01-30 | HIGH |\n| titanic | CONFIG_ONLY | LOW | Data Science Team | data-science-team@company.com | 2024-02-10 | HIGH |\n| pagila | MIXED | MEDIUM | Development Team | development-team@company.com | 2024-04-15 | MEDIUM |\n| chinook | MIXED | MEDIUM | Media Team | media-team@company.com | 2024-03-25 | MEDIUM ",
      "size": 0,
      "type": "",
      "matches": 0
    },
    {
      "path": "datadog_monitor_postgres_air.yaml",
      "content": "# monitoring/database-monitors/postgres_air_monitor.yaml\n# Datadog monitoring configuration for postgres_air database\n# Scenario: LOGIC_HEAVY | Criticality: CRITICAL\n\napi_version: v1\nkind: Monitor\nmetadata:\n  name: \"postgres_air-database-connection-monitor\"\n  tags:\n    - \"database:postgres_air\"\n    - \"scenario:logic_heavy\"\n    - \"criticality:critical\"\n    - \"environment:multi\"\n    - \"service:database-monitoring\"\n    - \"team:operations_team\"\n\nspec:\n  # Database connection monitoring\n  type: \"query alert\"\n\n  query: |\n    max(last_30m):max:postgresql.connections.active{database:postgres_air} by {host}\n\n  name: \"Postgres_Air Database - Unused Connection Alert\"\n\n  message: |\n    **üîç Database Decommissioning Candidate Detected**\n\n    Database: postgres_air\n    Scenario Type: LOGIC_HEAVY\n    Criticality: CRITICAL\n\n    **Alert Details:**\n    - No active connections detected for {{#is_alert}}{{ value }}{{/is_alert}} seconds\n    - Threshold: 86400 seconds (1.0 days)\n    - Owner: operations-team@",
      "size": 0,
      "type": "",
      "matches": 0
    },
    {
      "path": "deploy_scenarios.sh",
      "content": "#!/bin/bash\n# scripts/deploy-scenarios.sh\n# Automated deployment script for database decommissioning test scenarios\n\nset -euo pipefail\n\n# Script configuration\nSCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\nREPO_ROOT=\"$(dirname \"$SCRIPT_DIR\")\"\nLOG_FILE=\"$REPO_ROOT/deployment.log\"\n\n# Color codes for output\nRED='\\033[0;31m'\nGREEN='\\033[0;32m'\nYELLOW='\\033[1;33m'\nBLUE='\\033[0;34m'\nNC='\\033[0m' # No Color\n\n# Logging function\nlog() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\n# Error handling\nerror() {\n    echo -e \"${RED}ERROR: $1${NC}\" >&2\n    log \"ERROR: $1\"\n    exit 1\n}\n\n# Success message\nsuccess() {\n    echo -e \"${GREEN}‚úÖ $1${NC}\"\n    log \"SUCCESS: $1\"\n}\n\n# Warning message  \nwarning() {\n    echo -e \"${YELLOW}‚ö†Ô∏è  $1${NC}\"\n    log \"WARNING: $1\"\n}\n\n# Info message\ninfo() {\n    echo -e \"${BLUE}‚ÑπÔ∏è  $1${NC}\"\n    log \"INFO: $1\"\n}\n\n# Usage information\nusage() {\n    echo \"Usage: $0 <environment> [options]\"\n    echo \"\"\n    echo \"Environments:\"\n    echo \"  dev  ",
      "size": 0,
      "type": "",
      "matches": 0
    },
    {
      "path": "deployment_guide.md",
      "content": "# Database Decommissioning Test Scenarios - Deployment Guide\n\n## Overview\n\nThis guide provides step-by-step instructions for deploying the enhanced postgres-sample-dbs repository with database decommissioning test scenarios. The implementation creates realistic enterprise patterns for testing automated database decommissioning workflows.\n\n## Quick Start\n\n```bash\n# 1. Clone the repository\ngit clone https://github.com/bprzybys-nc/postgres-sample-dbs.git\ncd postgres-sample-dbs\n\n# 2. Validate scenario implementation\npython test_scenarios_validation.py\n\n# 3. Deploy scenarios (choose environment)\n./scripts/deploy-scenarios.sh dev\n```\n\n## Repository Structure\n\n```\npostgres-sample-dbs-enhanced/\n‚îú‚îÄ‚îÄ terraform/\n‚îÇ   ‚îú‚îÄ‚îÄ environments/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dev/databases.tf           # Config-Only scenarios\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prod/critical_databases.tf # Logic-Heavy scenarios\n‚îÇ   ‚îî‚îÄ‚îÄ modules/database/              # Reusable database module\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ config/database_connections.py # Mixed scenario configs\n‚îÇ ",
      "size": 0,
      "type": "",
      "matches": 0
    },
    {
      "path": "helm_values_postgres_air.yaml",
      "content": "# helm-charts/postgres_air/values.yaml\n# Kubernetes deployment configuration for postgres_air database\n# Scenario: LOGIC_HEAVY | Criticality: CRITICAL\n\n# Global configuration\nglobal:\n  database:\n    name: \"postgres_air\"\n    scenario: \"logic_heavy\"\n    criticality: \"critical\"\n\n  # Image registry settings\n  imageRegistry: \"registry.company.com\"\n  storageClass: \"premium-ssd\"\n\n# PostgreSQL configuration\npostgresql:\n  enabled: true\n\n  # Database connection settings\n  auth:\n    postgresPassword: \"\"  # Will be set via secret\n    username: \"dbuser\"\n    password: \"\"  # Will be set via secret\n    database: \"postgres_air\"\n\n  # Primary server configuration\n  primary:\n    name: \"primary\"\n\n    # Resource allocation based on criticality\n    resources:\n      requests:\n        cpu: '2000m'\n        memory: '4Gi'\n      limits:\n        cpu: '4000m'\n        memory: '8Gi'\n\n    # Storage configuration  \n    persistence:\n      enabled: true\n      size: 100Gi\n      storageClass: premium-ssd\n      accessModes:\n",
      "size": 0,
      "type": "",
      "matches": 0
    },
    {
      "path": "implementation_summary.md",
      "content": "# Implementation Summary: Database Decommissioning Test Scenarios\n\n## Overview\n\nSuccessfully implemented comprehensive test scenarios for the postgres-sample-dbs repository to simulate realistic database decommissioning workflows. The implementation follows enterprise patterns and includes Infrastructure as Code, application logic, monitoring, and documentation.\n\n## Implementation Status: ‚úÖ COMPLETE\n\n### ‚úÖ Core Requirements Fulfilled\n\n1. **‚úÖ Repository Enhancement**: Enhanced postgres-sample-dbs with decommissioning test scenarios\n2. **‚úÖ Three Scenario Types**: CONFIG_ONLY, MIXED, LOGIC_HEAVY properly separated\n3. **‚úÖ Infrastructure as Code**: Terraform configurations for all environments\n4. **‚úÖ Application Code**: Service layers and business logic by scenario type\n5. **‚úÖ Monitoring Setup**: Datadog configurations with 30+ day thresholds\n6. **‚úÖ Documentation**: Comprehensive ownership and deployment guides\n7. **‚úÖ Validation**: Automated scenario validation script\n8. **‚úÖ Deployment Read",
      "size": 0,
      "type": "",
      "matches": 0
    },
    {
      "path": "README.md",
      "content": "# postgres-sample-dbs\n\nA collection of sample Postgres databases for learning, testing, and development.\n\n# How the dataset files were created\n\nData was loaded into [Neon Serverless Postgres](https://neon.tech/) (Postgres 15). The data was then dumped using the [pg_dump](https://www.postgresql.org/docs/current/app-pgdump.html) utility. For example:\n\n```bash\npg_dump \"postgres://<user>:<password>@<hostname>/<dbname>\" --file=[file_name].sql --format=p --no-owner --no-privileges\n```\n\nFor larger datasets, such as the [employees](#employees-database) database, the following format option was used: `--format=c`\n\n### Clone the repository to your local machine\n\n```bash\ngit clone https://github.com/danieltprice/postgres-sample-dbs.git\n```\n\n### Download an individual dump file\n\nYou can download an individual dump file from this repo on the GitHub site or using `wget`.\n\nFrom this repo on the GitHub site:\n\n1. Click on the dump file to open it.\n2. Above the content of the file, you should see a butt",
      "size": 0,
      "type": "",
      "matches": 0
    },
    {
      "path": "script_1.py",
      "content": "# Create the Terraform configuration for production environment (Logic-Heavy scenario)\nterraform_prod_config = \"\"\"# terraform/environments/prod/critical_databases.tf\n# Logic-Heavy Database Scenarios for Production Environment\n# These databases have complex business logic and require manual review for decommissioning\n\nterraform {\n  required_providers {\n    azurerm = {\n      source  = \"hashicorp/azurerm\"\n      version = \"~>3.0\"\n    }\n    random = {\n      source  = \"hashicorp/random\"\n      version = \"~>3.1\"\n    }\n  }\n}\n\nprovider \"azurerm\" {\n  features {}\n}\n\n# Resource Group for Production Critical Databases\nresource \"azurerm_resource_group\" \"prod_critical_databases\" {\n  name     = \"rg-databases-prod-critical\"\n  location = \"East US\"\n  \n  tags = {\n    Environment = \"Production\"\n    Purpose     = \"Critical Business Operations\"\n    Owner       = \"Database Administration Team\"\n    LastUsed    = \"2025-06-24\"  # Current/Recent usage\n    Criticality = \"CRITICAL\"\n    Project     = \"Core Business S",
      "size": 0,
      "type": "",
      "matches": 0
    },
    {
      "path": "script_10.py",
      "content": "# Create comprehensive database ownership documentation\ndatabase_ownership_doc = \"\"\"# Database Ownership and Decommissioning Documentation\n\n## Overview\n\nThis document provides comprehensive ownership information for all databases in the postgres-sample-dbs testing environment, designed to simulate realistic database decommissioning workflows.\n\n## Database Inventory\n\n| Database | Scenario Type | Criticality | Owner Team | Contact Email | Last Used | Decommissioning Risk |\n|----------|---------------|-------------|------------|---------------|-----------|---------------------|\n| periodic_table | CONFIG_ONLY | LOW | Chemistry Team | chemistry-team@company.com | 2024-02-20 | HIGH |\n| world_happiness | CONFIG_ONLY | LOW | Analytics Team | analytics-team@company.com | 2024-01-30 | HIGH |\n| titanic | CONFIG_ONLY | LOW | Data Science Team | data-science-team@company.com | 2024-02-10 | HIGH |\n| pagila | MIXED | MEDIUM | Development Team | development-team@company.com | 2024-04-15 | MEDIUM |\n| c",
      "size": 0,
      "type": "",
      "matches": 0
    },
    {
      "path": "script_11.py",
      "content": "# Create comprehensive validation script for scenario implementation\nvalidation_script = '''#!/usr/bin/env python3\n\"\"\"\nTest Scenarios Validation Script\n================================\n\nValidates that database decommissioning test scenarios are properly implemented\naccording to the separation rules defined in the requirements.\n\nScenario Rules:\n- CONFIG_ONLY: References ONLY in Terraform, Helm, Docker, environment files\n- MIXED: Terraform + basic service connections (NO business logic)\n- LOGIC_HEAVY: Terraform + complex business operations + analytics\n\nAuthor: Database Team\nVersion: 1.0\n\"\"\"\n\nimport os\nimport re\nimport json\nimport yaml\nfrom pathlib import Path\nfrom typing import Dict, List, Set, Tuple, Any\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nclass ScenarioType(Enum):\n    CONFIG_ONLY = \"CONFIG_ONLY\"\n    MIXED = \"MIXED\"\n    LOGIC_HEAVY = \"LOGIC_HEAVY\"\n\nclass ViolationType(Enum):\n    CRITICAL = \"CRITICAL\"\n    WARNING = \"WARNING\"\n    INFO = \"INFO\"\n\n@dataclass\nclass Valid",
      "size": 0,
      "type": "",
      "matches": 0
    },
    {
      "path": "script_12.py",
      "content": "# Create the validation script again without the chmod operation\nvalidation_script = '''#!/usr/bin/env python3\n\"\"\"\nTest Scenarios Validation Script\n================================\n\nValidates that database decommissioning test scenarios are properly implemented\naccording to the separation rules defined in the requirements.\n\nUsage: python test_scenarios_validation.py\n\nAuthor: Database Team\nVersion: 1.0\n\"\"\"\n\nimport os\nimport re\nimport json\nfrom pathlib import Path\nfrom typing import Dict, List, Set, Tuple, Any\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nclass ScenarioType(Enum):\n    CONFIG_ONLY = \"CONFIG_ONLY\"\n    MIXED = \"MIXED\"\n    LOGIC_HEAVY = \"LOGIC_HEAVY\"\n\nclass ViolationType(Enum):\n    CRITICAL = \"CRITICAL\"\n    WARNING = \"WARNING\"\n    INFO = \"INFO\"\n\n@dataclass\nclass ValidationResult:\n    \"\"\"Validation result for a specific check\"\"\"\n    database: str\n    scenario: ScenarioType\n    check_name: str\n    status: str  # PASS, FAIL, WARNING\n    violation_type: ViolationType\n",
      "size": 0,
      "type": "",
      "matches": 0
    },
    {
      "path": "script_13.py",
      "content": "# Create comprehensive deployment guide\ndeployment_guide = \"\"\"# Database Decommissioning Test Scenarios - Deployment Guide\n\n## Overview\n\nThis guide provides step-by-step instructions for deploying the enhanced postgres-sample-dbs repository with database decommissioning test scenarios. The implementation creates realistic enterprise patterns for testing automated database decommissioning workflows.\n\n## Quick Start\n\n```bash\n# 1. Clone the repository\ngit clone https://github.com/bprzybys-nc/postgres-sample-dbs.git\ncd postgres-sample-dbs\n\n# 2. Validate scenario implementation\npython test_scenarios_validation.py\n\n# 3. Deploy scenarios (choose environment)\n./scripts/deploy-scenarios.sh dev\n```\n\n## Repository Structure\n\n```\npostgres-sample-dbs-enhanced/\n‚îú‚îÄ‚îÄ terraform/\n‚îÇ   ‚îú‚îÄ‚îÄ environments/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dev/databases.tf           # Config-Only scenarios\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prod/critical_databases.tf # Logic-Heavy scenarios\n‚îÇ   ‚îî‚îÄ‚îÄ modules/database/              # Reusable database module\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ",
      "size": 0,
      "type": "",
      "matches": 0
    },
    {
      "path": "script_14.py",
      "content": "# Create deployment script\ndeployment_script = \"\"\"#!/bin/bash\n# scripts/deploy-scenarios.sh\n# Automated deployment script for database decommissioning test scenarios\n\nset -euo pipefail\n\n# Script configuration\nSCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\nREPO_ROOT=\"$(dirname \"$SCRIPT_DIR\")\"\nLOG_FILE=\"$REPO_ROOT/deployment.log\"\n\n# Color codes for output\nRED='\\\\033[0;31m'\nGREEN='\\\\033[0;32m'\nYELLOW='\\\\033[1;33m'\nBLUE='\\\\033[0;34m'\nNC='\\\\033[0m' # No Color\n\n# Logging function\nlog() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\n# Error handling\nerror() {\n    echo -e \"${RED}ERROR: $1${NC}\" >&2\n    log \"ERROR: $1\"\n    exit 1\n}\n\n# Success message\nsuccess() {\n    echo -e \"${GREEN}‚úÖ $1${NC}\"\n    log \"SUCCESS: $1\"\n}\n\n# Warning message  \nwarning() {\n    echo -e \"${YELLOW}‚ö†Ô∏è  $1${NC}\"\n    log \"WARNING: $1\"\n}\n\n# Info message\ninfo() {\n    echo -e \"${BLUE}‚ÑπÔ∏è  $1${NC}\"\n    log \"INFO: $1\"\n}\n\n# Usage information\nusage() {\n    echo \"Usage: $0 <environment> [options]\"",
      "size": 0,
      "type": "",
      "matches": 0
    },
    {
      "path": "script_15.py",
      "content": "# Create a comprehensive implementation summary\nimplementation_summary = \"\"\"# Implementation Summary: Database Decommissioning Test Scenarios\n\n## Overview\n\nSuccessfully implemented comprehensive test scenarios for the postgres-sample-dbs repository to simulate realistic database decommissioning workflows. The implementation follows enterprise patterns and includes Infrastructure as Code, application logic, monitoring, and documentation.\n\n## Implementation Status: ‚úÖ COMPLETE\n\n### ‚úÖ Core Requirements Fulfilled\n\n1. **‚úÖ Repository Enhancement**: Enhanced postgres-sample-dbs with decommissioning test scenarios\n2. **‚úÖ Three Scenario Types**: CONFIG_ONLY, MIXED, LOGIC_HEAVY properly separated\n3. **‚úÖ Infrastructure as Code**: Terraform configurations for all environments\n4. **‚úÖ Application Code**: Service layers and business logic by scenario type\n5. **‚úÖ Monitoring Setup**: Datadog configurations with 30+ day thresholds\n6. **‚úÖ Documentation**: Comprehensive ownership and deployment guides\n7. *",
      "size": 0,
      "type": "",
      "matches": 0
    },
    {
      "path": "script_8.py",
      "content": "# Create Datadog monitoring configurations for all databases\ndef create_datadog_monitor(database_name, scenario_type, owner_email, criticality):\n    \"\"\"Create Datadog monitor configuration for a database\"\"\"\n\n    # Set thresholds based on criticality\n    if criticality == \"CRITICAL\":\n        connection_threshold = 86400  # 24 hours\n        warning_threshold = 43200  # 12 hours\n    elif criticality == \"MEDIUM\":\n        connection_threshold = 259200  # 72 hours (3 days)\n        warning_threshold = 172800  # 48 hours (2 days)\n    else:  # LOW\n        connection_threshold = 2592000  # 30 days\n        warning_threshold = 1814400  # 21 days\n\n    monitor_config = f\"\"\"# monitoring/database-monitors/{database_name}_monitor.yaml\n# Datadog monitoring configuration for {database_name} database\n# Scenario: {scenario_type} | Criticality: {criticality}\n\napi_version: v1\nkind: Monitor\nmetadata:\n  name: \"{database_name}-database-connection-monitor\"\n  tags:\n    - \"database:{database_name}\"\n    - \"scenario",
      "size": 0,
      "type": "",
      "matches": 0
    },
    {
      "path": "script_9.py",
      "content": "# Create Helm chart configurations for each database\ndef create_helm_values(database_name, scenario_type, criticality, description):\n    \"\"\"Create Helm values.yaml for a database\"\"\"\n\n    helm_values = f\"\"\"# helm-charts/{database_name}/values.yaml\n# Kubernetes deployment configuration for {database_name} database\n# Scenario: {scenario_type} | Criticality: {criticality}\n\n# Global configuration\nglobal:\n  database:\n    name: \"{database_name}\"\n    scenario: \"{scenario_type.lower()}\"\n    criticality: \"{criticality.lower()}\"\n  \n  # Image registry settings\n  imageRegistry: \"registry.company.com\"\n  storageClass: \"premium-ssd\"\n  \n# PostgreSQL configuration\npostgresql:\n  enabled: true\n  \n  # Database connection settings\n  auth:\n    postgresPassword: \"\"  # Will be set via secret\n    username: \"dbuser\"\n    password: \"\"  # Will be set via secret\n    database: \"{database_name}\"\n  \n  # Primary server configuration\n  primary:\n    name: \"primary\"\n    \n    # Resource allocation based on criticality\n    r",
      "size": 0,
      "type": "",
      "matches": 0
    },
    {
      "path": "terraform_prod_critical_databases.tf",
      "content": "# terraform/environments/prod/critical_databases.tf\n# Logic-Heavy Database Scenarios for Production Environment\n# These databases have complex business logic and require manual review for decommissioning\n\nterraform {\n  required_providers {\n    azurerm = {\n      source  = \"hashicorp/azurerm\"\n      version = \"~>3.0\"\n    }\n    random = {\n      source  = \"hashicorp/random\"\n      version = \"~>3.1\"\n    }\n  }\n}\n\nprovider \"azurerm\" {\n  features {}\n}\n\n# Resource Group for Production Critical Databases\nresource \"azurerm_resource_group\" \"prod_critical_databases\" {\n  name     = \"rg-databases-prod-critical\"\n  location = \"East US\"\n\n  tags = {\n    Environment = \"Production\"\n    Purpose     = \"Critical Business Operations\"\n    Owner       = \"Database Administration Team\"\n    LastUsed    = \"2025-06-24\"  # Current/Recent usage\n    Criticality = \"CRITICAL\"\n    Project     = \"Core Business Systems\"\n    ComplianceLevel = \"SOX\"\n  }\n}\n\n# Random password for database administrator\nresource \"random_password\" \"",
      "size": 0,
      "type": "",
      "matches": 0
    },
    {
      "path": "test_scenarios_validation.py",
      "content": "#!/usr/bin/env python3\n\"\"\"\nTest Scenarios Validation Script\n================================\n\nValidates that database decommissioning test scenarios are properly implemented\naccording to the separation rules defined in the requirements.\n\nUsage: python test_scenarios_validation.py\n\nAuthor: Database Team\nVersion: 1.0\n\"\"\"\n\nimport os\nimport re\nimport json\nfrom pathlib import Path\nfrom typing import Dict, List, Set, Tuple, Any\nfrom dataclasses import dataclass\nfrom enum import Enum\n\n\nclass ScenarioType(Enum):\n    CONFIG_ONLY = \"CONFIG_ONLY\"\n    MIXED = \"MIXED\"\n    LOGIC_HEAVY = \"LOGIC_HEAVY\"\n\n\nclass ViolationType(Enum):\n    CRITICAL = \"CRITICAL\"\n    WARNING = \"WARNING\"\n    INFO = \"INFO\"\n\n\n@dataclass\nclass ValidationResult:\n    \"\"\"Validation result for a specific check\"\"\"\n\n    database: str\n    scenario: ScenarioType\n    check_name: str\n    status: str  # PASS, FAIL, WARNING\n    violation_type: ViolationType\n    message: str\n    details: List[str]\n    file_references: List[str]\n\n\nclass Datab",
      "size": 0,
      "type": "",
      "matches": 0
    }
  ]
}